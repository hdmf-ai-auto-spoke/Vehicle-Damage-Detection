{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMvcx0qmaTypA41X4iHxcA8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s2ipbn-Mn_8k","executionInfo":{"status":"ok","timestamp":1766306015257,"user_tz":-540,"elapsed":17956,"user":{"displayName":"Jae-rak Lim","userId":"07389028510183528444"}},"outputId":"3b1306da-e38d-44ec-d89e-3cd64dbb288a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os, glob\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","import matplotlib.pyplot as plt\n","\n","# =========================\n","# 0) 사용자가 바꿀 값\n","# =========================\n","NORMAL_FOLDER = \"/content/gdrive/MyDrive/01.DS Part/99.Study/01.Vehicle_Damage_Detection/00. Datasets/Kaggle(Car damage detection)/training/01-whole\"   # ✅ 정상차량만 모아둔 폴더\n","MODEL_PATH    = \"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/SUBJECT/WEEK1_CAR_DETECTION/JR/01.Model/damage_classifier_stage2(20epoch).pth\"  # ✅ Stage2 저장 모델(state_dict)\n","BATCH_SIZE    = 64\n","NUM_WORKERS   = 2\n","IMG_SIZE      = 224\n","\n","# 노트북 기준 라벨(중요!)\n","NORMAL_CLASS_IDX  = 0   # normal = 0\n","DAMAGED_CLASS_IDX = 1   # damaged = 1\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","\n","# =========================\n","# 1) 노트북의 val/test transform 그대로\n","# =========================\n","val_test_transform = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.CenterCrop(IMG_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std =[0.229, 0.224, 0.225],\n","    ),\n","])\n","\n","\n","# =========================\n","# 2) 폴더 이미지 Dataset (이미지 + 경로 반환)\n","# =========================\n","class FolderImageDataset(Dataset):\n","    def __init__(self, folder, transform=None):\n","        exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\", \"*.JPEG\", \"*.JPG\")\n","        paths = []\n","        for e in exts:\n","            paths.extend(glob.glob(os.path.join(folder, e)))\n","        self.paths = sorted(paths)\n","        if len(self.paths) == 0:\n","            raise ValueError(f\"No images found in: {folder}\")\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, idx):\n","        path = self.paths[idx]\n","        img = Image.open(path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, path\n","\n","\n","# =========================\n","# 3) 노트북과 동일하게 ResNet18(2-class) 구성 후 state_dict 로드\n","#    (학습 때 pretrained=True였지만, state_dict 로드가 덮어쓰므로 weights=None로 생성해도 보통 문제 없습니다)\n","# =========================\n","def build_resnet18_2class():\n","    model = models.resnet18(weights=None)\n","    in_features = model.fc.in_features\n","    model.fc = nn.Linear(in_features, 2)\n","    return model\n","\n","model = build_resnet18_2class()\n","state = torch.load(MODEL_PATH, map_location=device)\n","model.load_state_dict(state)\n","model.to(device)\n","model.eval()\n","\n","\n","# =========================\n","# 4) 추론 + \"정상 폴더\" 기준 정확도 계산\n","# =========================\n","ds = FolderImageDataset(NORMAL_FOLDER, transform=val_test_transform)\n","dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n","\n","total = 0\n","correct = 0\n","pred_normal = 0\n","pred_damaged = 0\n","mis_paths = []   # 정상인데 damaged로 예측된 이미지 목록(오답)\n","\n","with torch.no_grad():\n","    for images, paths in dl:\n","        images = images.to(device)\n","\n","        logits = model(images)                 # [B,2]\n","        probs  = torch.softmax(logits, dim=1)  # [B,2]\n","        preds  = torch.argmax(probs, dim=1)    # [B]\n","\n","        preds = preds.cpu().numpy()\n","\n","        for p, path in zip(preds, paths):\n","            total += 1\n","            if int(p) == NORMAL_CLASS_IDX:\n","                pred_normal += 1\n","                correct += 1\n","            else:\n","                pred_damaged += 1\n","                mis_paths.append(path)\n","\n","acc = correct / total if total else 0.0\n","\n","print(\"\\n==== Normal-folder Evaluation (Ground truth: ALL NORMAL) ====\")\n","print(f\"Total images           : {total}\")\n","print(f\"Pred NORMAL (correct)  : {pred_normal}\")\n","print(f\"Pred DAMAGED (wrong)   : {pred_damaged}\")\n","print(f\"Accuracy               : {acc:.4f} ({correct}/{total})\")\n","\n","print(\"\\n[Wrong samples] normal folder but predicted DAMAGED (show up to 30):\")\n","for p in mis_paths[:30]:\n","    print(\" -\", p)\n","\n","\n","# =========================\n","# 5) (선택) 오답 이미지 몇 장 바로 보기\n","# =========================\n","def show_images(paths, max_show=16, cols=4, title=\"Misclassified (pred=damaged)\"):\n","    if len(paths) == 0:\n","        print(\"No misclassified images to show.\")\n","        return\n","    n = min(max_show, len(paths))\n","    rows = (n + cols - 1) // cols\n","    plt.figure(figsize=(4*cols, 4*rows))\n","    for i in range(n):\n","        img = Image.open(paths[i]).convert(\"RGB\")\n","        plt.subplot(rows, cols, i+1)\n","        plt.imshow(img)\n","        plt.axis(\"off\")\n","        plt.title(os.path.basename(paths[i]), fontsize=9)\n","    plt.suptitle(title)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# 오답 이미지 확인\n","show_images(mis_paths, max_show=16, cols=4)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ogJOSGh4uXVBWzn25xbWvoXq4Hc1Lvo7"},"id":"viaKwKrxoGX0","executionInfo":{"status":"ok","timestamp":1766306113542,"user_tz":-540,"elapsed":48619,"user":{"displayName":"Jae-rak Lim","userId":"07389028510183528444"}},"outputId":"b3bf47ec-69c3-44df-957b-fe8eb2fcd259"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import os, glob\n","from PIL import Image\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models, transforms\n","import matplotlib.pyplot as plt\n","\n","# =========================\n","# 0) 사용자가 바꿀 값\n","# =========================\n","DAMAGED_FOLDER = \"/content/gdrive/MyDrive/01.DS Part/99.Study/01.Vehicle_Damage_Detection/00. Datasets/Kaggle(Car damage detection)/training/00-damage\"     # ✅ 파손차량만 모아둔 폴더\n","MODEL_PATH     = \"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/SUBJECT/WEEK1_CAR_DETECTION/JR/01.Model/damage_classifier_stage2(20epoch).pth\"  # ✅ Stage2 모델(state_dict)\n","BATCH_SIZE     = 64\n","NUM_WORKERS    = 2\n","IMG_SIZE       = 224\n","\n","# 노트북 기준 라벨(중요!)\n","NORMAL_CLASS_IDX  = 0   # normal = 0\n","DAMAGED_CLASS_IDX = 1   # damaged = 1\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# =========================\n","# 1) 노트북의 val/test transform 그대로\n","# =========================\n","val_test_transform = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.CenterCrop(IMG_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std =[0.229, 0.224, 0.225],\n","    ),\n","])\n","\n","# =========================\n","# 2) 폴더 이미지 Dataset (이미지 + 경로 반환)\n","# =========================\n","class FolderImageDataset(Dataset):\n","    def __init__(self, folder, transform=None):\n","        exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.webp\", \"*.JPEG\")\n","        paths = []\n","        for e in exts:\n","            paths.extend(glob.glob(os.path.join(folder, e)))\n","        self.paths = sorted(paths)\n","        if len(self.paths) == 0:\n","            raise ValueError(f\"No images found in: {folder}\")\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, idx):\n","        path = self.paths[idx]\n","        img = Image.open(path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, path\n","\n","# =========================\n","# 3) ResNet18(2-class) 구성 후 state_dict 로드\n","# =========================\n","def build_resnet18_2class():\n","    model = models.resnet18(weights=None)\n","    in_features = model.fc.in_features\n","    model.fc = nn.Linear(in_features, 2)\n","    return model\n","\n","model = build_resnet18_2class()\n","state = torch.load(MODEL_PATH, map_location=device)\n","model.load_state_dict(state)\n","model.to(device)\n","model.eval()\n","\n","# =========================\n","# 4) 추론 + \"파손 폴더\" 기준 정확도 계산\n","#    - 정답은 전부 DAMAGED(1)\n","#    - 틀린 것: pred == NORMAL(0)  => (FN 개념)\n","# =========================\n","ds = FolderImageDataset(DAMAGED_FOLDER, transform=val_test_transform)\n","dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n","\n","total = 0\n","correct = 0\n","pred_normal = 0\n","pred_damaged = 0\n","\n","wrong_paths = []   # 파손 폴더인데 normal로 예측된 이미지 목록 (=오답/FN)\n","\n","with torch.no_grad():\n","    for images, paths in dl:\n","        images = images.to(device)\n","\n","        logits = model(images)                 # [B,2]\n","        probs  = torch.softmax(logits, dim=1)  # [B,2]\n","        preds  = torch.argmax(probs, dim=1)    # [B]\n","\n","        preds = preds.cpu().numpy()\n","        for p, path in zip(preds, paths):\n","            total += 1\n","            if int(p) == DAMAGED_CLASS_IDX:\n","                pred_damaged += 1\n","                correct += 1\n","            else:\n","                pred_normal += 1\n","                wrong_paths.append(path)\n","\n","acc = correct / total if total else 0.0\n","\n","print(\"\\n==== Damaged-folder Evaluation (Ground truth: ALL DAMAGED) ====\")\n","print(f\"Total images           : {total}\")\n","print(f\"Pred DAMAGED (correct) : {pred_damaged}\")\n","print(f\"Pred NORMAL (wrong/FN) : {pred_normal}\")\n","print(f\"Accuracy               : {acc:.4f} ({correct}/{total})\")\n","\n","print(\"\\n[Wrong samples] damaged folder but predicted NORMAL (show up to 30):\")\n","for p in wrong_paths[:30]:\n","    print(\" -\", p)\n","\n","# =========================\n","# 5) (선택) 오답 이미지 몇 장 바로 보기\n","# =========================\n","def show_images(paths, max_show=16, cols=4, title=\"Wrong (pred=normal) from damaged folder\"):\n","    if len(paths) == 0:\n","        print(\"No wrong images to show.\")\n","        return\n","    n = min(max_show, len(paths))\n","    rows = (n + cols - 1) // cols\n","    plt.figure(figsize=(4*cols, 4*rows))\n","    for i in range(n):\n","        img = Image.open(paths[i]).convert(\"RGB\")\n","        plt.subplot(rows, cols, i+1)\n","        plt.imshow(img)\n","        plt.axis(\"off\")\n","        plt.title(os.path.basename(paths[i]), fontsize=9)\n","    plt.suptitle(title)\n","    plt.tight_layout()\n","    plt.show()\n","\n","show_images(wrong_paths, max_show=16, cols=4)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1baI54jt2J9EUDgFEsFYf1VIPbVs5qKSO"},"id":"X-7J4lqWpjax","executionInfo":{"status":"ok","timestamp":1766306156713,"user_tz":-540,"elapsed":21753,"user":{"displayName":"Jae-rak Lim","userId":"07389028510183528444"}},"outputId":"693f3db2-ed12-4b64-e9ac-6f6e98a75a73"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"B8BxMi5op573"},"execution_count":null,"outputs":[]}]}