{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25930,"status":"ok","timestamp":1766529073045,"user":{"displayName":"Jae-rak Lim","userId":"07389028510183528444"},"user_tz":-540},"id":"8aSJZ-x9HFut","outputId":"4eda73a6-089c-4c89-f18e-a4e30c54459e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# 차량 파손 여부(정상 vs 파손) 분류 파이프라인\n","# 요구사항\n","# 1) YOLO pretrained(탐지)로 \"차량이 이미지의 50% 이상\"인 샘플만 선별\n","# 2) 선별된 데이터로 (A) YOLO pretrained 기반 분류모델, (B) ResNet pretrained 기반 분류모델 둘 다 학습\n","# 3) test에서 accuracy / f1 / confusion matrix 보기 좋게 출력\n","# 4) test에서 TN, FN 샘플을 각 최대 5개 시각화\n","\n","# =========================\n","# 0. 설치/임포트\n","# =========================\n","!pip -q install ultralytics\n","\n","import os, random, shutil, math\n","from pathlib import Path\n","from collections import defaultdict\n","\n","import numpy as np\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","\n","import matplotlib.pyplot as plt\n","\n","from ultralytics import YOLO\n","\n","# 재현성(선택)\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"DEVICE:\", DEVICE)\n","\n","# =========================\n","# 1. 사용자 경로 설정\n","# =========================\n","# 아래 4개 폴더는 \"이미지 파일들이 바로 들어있는\" 폴더라고 가정합니다.\n","NORMAL_DIRS = [\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/normal/images\"),\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/normal(kaggle_dataset)\"),\n","]\n","DAMAGED_DIRS = [\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/damaged/images\"),\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/damaged(kaggle_dataset)\"),\n","]\n","\n","# 필터링/학습용 임시 데이터셋 생성 위치\n","WORKDIR = Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/01.Vehicle_Damage_Detection/Week2_Study\")\n","FILTERED_RAW = WORKDIR / \"filtered_raw\"     # 필터링된 원본 복사본\n","CLS_DATASET  = WORKDIR / \"cls_dataset\"      # ImageFolder 구조( train/val/test / normal|damaged )\n","YOLO_RUNS    = WORKDIR / \"yolo_runs\"\n","\n","# 데이터 split 비율\n","TRAIN_RATIO = 0.7\n","VAL_RATIO   = 0.15\n","TEST_RATIO  = 0.15\n","\n","# 차량이 차지해야 하는 최소 비율\n","MIN_CAR_RATIO = 0.50\n","\n","# YOLO 탐지 모델(Pretrained)\n","YOLO_DET_WEIGHTS = \"yolov8n.pt\"   # 필요 시 yolov8s.pt 등으로 변경\n","\n","# YOLO 분류 모델(Pretrained)\n","YOLO_CLS_WEIGHTS = \"yolov8n-cls.pt\"  # 필요 시 yolov8s-cls.pt 등으로 변경\n","\n","# 학습 하이퍼파라미터(필요 시 조정)\n","YOLO_EPOCHS = 20\n","YOLO_IMGSZ  = 224     # 분류는 보통 224/256\n","YOLO_BATCH  = 32\n","\n","RESNET_EPOCHS = 10\n","RESNET_BATCH  = 32\n","RESNET_LR     = 1e-3\n","IMG_SIZE      = 224   # ResNet 입력 크기\n","\n","# =========================\n","# 2. 유틸: 이미지 파일 수집\n","# =========================\n","IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".JPEG\",\".JPG\",\".webp\"}\n","\n","def list_images(dir_path: Path):\n","    if not dir_path.exists():\n","        print(f\"[WARN] Not found: {dir_path}\")\n","        return []\n","    return [p for p in dir_path.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n","\n","normal_imgs = []\n","damaged_imgs = []\n","for d in NORMAL_DIRS:\n","    normal_imgs += list_images(d)\n","for d in DAMAGED_DIRS:\n","    damaged_imgs += list_images(d)\n","\n","print(\"normal:\", len(normal_imgs))\n","print(\"damaged:\", len(damaged_imgs))\n","\n","assert len(normal_imgs) > 0 and len(damaged_imgs) > 0, \"이미지 경로를 확인해 주세요.\"\n","\n","# =========================\n","# 3. 1단계: YOLO 탐지로 '차량 비율 >= 50%'만 선별\n","# =========================\n","det_model = YOLO(YOLO_DET_WEIGHTS)\n","\n","# COCO 기준 차량 관련 클래스(필요 시 확장)\n","# yolov8 COCO names: car, motorcycle, bus, truck 등\n","VEHICLE_NAMES = {\"car\", \"bus\", \"truck\"}  # 'train' 같은 건 제외\n","\n","def get_vehicle_ratio_yolo(image_path: Path):\n","    \"\"\"\n","    YOLO 탐지 결과 중 차량 클래스의 bbox 중 가장 큰 bbox 기준으로\n","    bbox_area / image_area 반환 (차량이 하나도 없으면 0.0)\n","    \"\"\"\n","    try:\n","        im = Image.open(image_path).convert(\"RGB\")\n","    except Exception:\n","        return 0.0\n","\n","    w, h = im.size\n","    img_area = float(w * h)\n","\n","    # stream=False로 단일 이미지 예측\n","    res = det_model.predict(source=str(image_path), conf=0.25, iou=0.5, verbose=False)\n","    r = res[0]\n","\n","    if r.boxes is None or len(r.boxes) == 0:\n","        return 0.0\n","\n","    names = r.names  # {id: name}\n","    best_area = 0.0\n","\n","    # boxes: xyxy, cls\n","    xyxy = r.boxes.xyxy.cpu().numpy()\n","    cls  = r.boxes.cls.cpu().numpy().astype(int)\n","\n","    for (x1, y1, x2, y2), c in zip(xyxy, cls):\n","        name = names.get(int(c), \"\")\n","        if name in VEHICLE_NAMES:\n","            bw = max(0.0, x2 - x1)\n","            bh = max(0.0, y2 - y1)\n","            area = bw * bh\n","            if area > best_area:\n","                best_area = area\n","\n","    return best_area / img_area if img_area > 0 else 0.0\n","\n","def filter_images_by_vehicle_ratio(image_paths, min_ratio=0.5):\n","    kept = []\n","    dropped = []\n","    for i, p in enumerate(image_paths, 1):\n","        ratio = get_vehicle_ratio_yolo(p)\n","        if ratio >= min_ratio:\n","            kept.append((p, ratio))\n","        else:\n","            dropped.append((p, ratio))\n","        if i % 200 == 0:\n","            print(f\"  processed {i}/{len(image_paths)} ... kept={len(kept)}\")\n","    return kept, dropped\n","\n","# 작업 폴더 준비\n","if WORKDIR.exists():\n","    shutil.rmtree(WORKDIR)\n","FILTERED_RAW.mkdir(parents=True, exist_ok=True)\n","\n","print(\"\\n[Filter] normal ...\")\n","kept_normal, drop_normal = filter_images_by_vehicle_ratio(normal_imgs, MIN_CAR_RATIO)\n","print(\"\\n[Filter] damaged ...\")\n","kept_damaged, drop_damaged = filter_images_by_vehicle_ratio(damaged_imgs, MIN_CAR_RATIO)\n","\n","print(\"\\n==== Filter Summary ====\")\n","print(f\"Normal kept:  {len(kept_normal)} / {len(normal_imgs)}\")\n","print(f\"Damaged kept: {len(kept_damaged)} / {len(damaged_imgs)}\")\n","\n","assert len(kept_normal) > 0 and len(kept_damaged) > 0, \"필터 조건이 너무 빡셉니다(차량 50%). MIN_CAR_RATIO를 낮춰보세요.\"\n","\n","# =========================\n","# 4. ImageFolder 구조(train/val/test + class 폴더)로 복사 + split\n","# =========================\n","def split_list(items, train_ratio, val_ratio, test_ratio):\n","    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n","    items = items[:]\n","    random.shuffle(items)\n","    n = len(items)\n","    n_train = int(n * train_ratio)\n","    n_val   = int(n * val_ratio)\n","    train = items[:n_train]\n","    val   = items[n_train:n_train+n_val]\n","    test  = items[n_train+n_val:]\n","    return train, val, test\n","\n","def safe_copy(src: Path, dst: Path):\n","    dst.parent.mkdir(parents=True, exist_ok=True)\n","    # 파일명 충돌 방지\n","    if dst.exists():\n","        stem = dst.stem\n","        suf = dst.suffix\n","        k = 1\n","        while True:\n","            cand = dst.with_name(f\"{stem}_{k}{suf}\")\n","            if not cand.exists():\n","                dst = cand\n","                break\n","            k += 1\n","    shutil.copy2(src, dst)\n","\n","def build_imagefolder_dataset(kept_normal, kept_damaged):\n","    # (path, ratio) -> path만\n","    normal_paths = [p for p, _ in kept_normal]\n","    damaged_paths = [p for p, _ in kept_damaged]\n","\n","    n_tr, n_va, n_te = split_list(normal_paths, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n","    d_tr, d_va, d_te = split_list(damaged_paths, TRAIN_RATIO, VAL_RATIO, TEST_RATIO)\n","\n","    for split_name, paths, cls_name in [\n","        (\"train\", n_tr, \"normal\"),\n","        (\"val\",   n_va, \"normal\"),\n","        (\"test\",  n_te, \"normal\"),\n","        (\"train\", d_tr, \"damaged\"),\n","        (\"val\",   d_va, \"damaged\"),\n","        (\"test\",  d_te, \"damaged\"),\n","    ]:\n","        out_dir = CLS_DATASET / split_name / cls_name\n","        out_dir.mkdir(parents=True, exist_ok=True)\n","        for src in paths:\n","            safe_copy(src, out_dir / src.name)\n","\n","    print(\"\\n[Dataset built]\")\n","    for split in [\"train\", \"val\", \"test\"]:\n","        for cls in [\"normal\", \"damaged\"]:\n","            cnt = len(list((CLS_DATASET / split / cls).glob(\"*\")))\n","            print(f\"  {split}/{cls}: {cnt}\")\n","\n","build_imagefolder_dataset(kept_normal, kept_damaged)"],"metadata":{"id":"Qebt_JkqEIzH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 차량 파손 여부(정상 vs 파손) 분류 파이프라인\n","# 요구사항\n","# 1) YOLO pretrained(탐지)로 \"차량이 이미지의 50% 이상\"인 샘플만 선별\n","# 2) 선별된 데이터로 (A) YOLO pretrained 기반 분류모델, (B) ResNet pretrained 기반 분류모델 둘 다 학습\n","# 3) test에서 accuracy / f1 / confusion matrix 보기 좋게 출력\n","# 4) test에서 TN, FN 샘플을 각 최대 5개 시각화\n","\n","# =========================\n","# 0. 설치/임포트\n","# =========================\n","!pip -q install ultralytics\n","\n","import os, random, shutil, math\n","from pathlib import Path\n","from collections import defaultdict\n","\n","import numpy as np\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","\n","import matplotlib.pyplot as plt\n","\n","from ultralytics import YOLO\n","\n","# 재현성(선택)\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"DEVICE:\", DEVICE)\n","\n","# =========================\n","# 1. 사용자 경로 설정\n","# =========================\n","# 아래 4개 폴더는 \"이미지 파일들이 바로 들어있는\" 폴더라고 가정합니다.\n","NORMAL_DIRS = [\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/normal/images\"),\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/normal(kaggle_dataset)\"),\n","]\n","DAMAGED_DIRS = [\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/damaged/images\"),\n","    Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/(share)HDMF_AUTO_SPOKE/DATA/(share)2026_ImageDetectionStudy_No_resizing/damaged(kaggle_dataset)\"),\n","]\n","\n","# 필터링/학습용 임시 데이터셋 생성 위치\n","WORKDIR = Path(\"/content/gdrive/MyDrive/01.DS Part/99.Study/01.Vehicle_Damage_Detection/Week2_Study\")\n","FILTERED_RAW = WORKDIR / \"filtered_raw\"     # 필터링된 원본 복사본\n","CLS_DATASET  = WORKDIR / \"cls_dataset\"      # ImageFolder 구조( train/val/test / normal|damaged )\n","YOLO_RUNS    = WORKDIR / \"yolo_runs\"\n","\n","# 데이터 split 비율\n","TRAIN_RATIO = 0.7\n","VAL_RATIO   = 0.15\n","TEST_RATIO  = 0.15\n","\n","# 차량이 차지해야 하는 최소 비율\n","MIN_CAR_RATIO = 0.50\n","\n","# YOLO 탐지 모델(Pretrained)\n","YOLO_DET_WEIGHTS = \"yolov8n.pt\"   # 필요 시 yolov8s.pt 등으로 변경\n","\n","# YOLO 분류 모델(Pretrained)\n","YOLO_CLS_WEIGHTS = \"yolov8n-cls.pt\"  # 필요 시 yolov8s-cls.pt 등으로 변경\n","\n","# 학습 하이퍼파라미터(필요 시 조정)\n","YOLO_EPOCHS = 20\n","YOLO_IMGSZ  = 224     # 분류는 보통 224/256\n","YOLO_BATCH  = 32\n","\n","RESNET_EPOCHS = 10\n","RESNET_BATCH  = 32\n","RESNET_LR     = 1e-3\n","IMG_SIZE      = 224   # ResNet 입력 크기\n","\n","# =========================\n","# 5-A. YOLO 분류모델 학습 (Ultralytics)\n","# =========================\n","yolo_cls = YOLO(YOLO_CLS_WEIGHTS)\n","\n","# Ultralytics classify는 data=에 \"train 폴더\"를 넣으면 (val 포함) 학습합니다.\n","# 여기서는 CLS_DATASET/train 사용 + val은 자동으로 CLS_DATASET/val을 찾기도 하지만,\n","# 가장 안전하게는 data=CLS_DATASET 로 루트 구조를 주는 방식이 편합니다.\n","# (Ultralytics 버전에 따라 동작 차이가 있을 수 있어, 아래는 호환성 높은 방식)\n","\n","yolo_exp_dir = YOLO_RUNS / \"yolo_cls\"\n","yolo_results = yolo_cls.train(\n","    data=str(CLS_DATASET),   # 루트에 train/val/test가 있으면 분류에서 인식 가능\n","    epochs=YOLO_EPOCHS,\n","    imgsz=YOLO_IMGSZ,\n","    batch=YOLO_BATCH,\n","    project=str(YOLO_RUNS),\n","    name=\"yolo_cls\",\n","    verbose=True\n",")\n","\n","# 가장 좋은 weights 경로(보통 runs/classify/.../weights/best.pt)\n","yolo_best = yolo_cls.trainer.best\n","print(\"YOLO best weights:\", yolo_best)\n","\n","# =========================\n","# 5-B. ResNet(사전학습) 분류모델 학습 (Torchvision)\n","# =========================\n","train_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n","])\n","eval_tf = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n","])\n","\n","train_ds = datasets.ImageFolder(CLS_DATASET / \"train\", transform=train_tf)\n","val_ds   = datasets.ImageFolder(CLS_DATASET / \"val\",   transform=eval_tf)\n","test_ds  = datasets.ImageFolder(CLS_DATASET / \"test\",  transform=eval_tf)\n","\n","class_to_idx = train_ds.class_to_idx  # {'damaged':0, 'normal':1} 처럼 될 수 있음\n","idx_to_class = {v:k for k,v in class_to_idx.items()}\n","print(\"class_to_idx:\", class_to_idx)\n","\n","train_loader = DataLoader(train_ds, batch_size=RESNET_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n","val_loader   = DataLoader(val_ds,   batch_size=RESNET_BATCH, shuffle=False, num_workers=2, pin_memory=True)\n","test_loader  = DataLoader(test_ds,  batch_size=RESNET_BATCH, shuffle=False, num_workers=2, pin_memory=True)\n","\n","resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","resnet.fc = nn.Linear(resnet.fc.in_features, 2)\n","resnet = resnet.to(DEVICE)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(resnet.parameters(), lr=RESNET_LR)\n","\n","def run_epoch(model, loader, train=False):\n","    model.train(train)\n","    total_loss, total_correct, total = 0.0, 0, 0\n","    for x, y in loader:\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","        if train:\n","            optimizer.zero_grad()\n","        logits = model(x)\n","        loss = criterion(logits, y)\n","        if train:\n","            loss.backward()\n","            optimizer.step()\n","\n","        total_loss += loss.item() * x.size(0)\n","        pred = logits.argmax(1)\n","        total_correct += (pred == y).sum().item()\n","        total += x.size(0)\n","    return total_loss/total, total_correct/total\n","\n","best_val_acc = -1\n","best_path = WORKDIR / \"resnet18_best.pt\"\n","\n","for epoch in range(1, RESNET_EPOCHS+1):\n","    tr_loss, tr_acc = run_epoch(resnet, train_loader, train=True)\n","    va_loss, va_acc = run_epoch(resnet, val_loader,   train=False)\n","    print(f\"[ResNet] epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n","    if va_acc > best_val_acc:\n","        best_val_acc = va_acc\n","        torch.save(resnet.state_dict(), best_path)\n","\n","print(\"Best val acc:\", best_val_acc, \"saved:\", best_path)"],"metadata":{"id":"WJxmx0jsGjsy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","output_embedded_package_id":"1yZcI8PywfWveXpYy3UKT3lEx86Q8Ts9y"},"id":"NWS0B0uvHKjk","outputId":"070458d2-94b7-4bd1-8d4e-6e88b9dd3047"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# =========================\n","# 6. 평가 지표(accuracy, f1, confusion matrix) 공통 함수\n","# =========================\n","def confusion_matrix_binary(y_true, y_pred, positive_label):\n","    \"\"\"\n","    positive_label: 'damaged' 같은 양성 클래스 이름\n","    \"\"\"\n","    # 라벨을 0/1로\n","    y_true_bin = np.array([1 if t==positive_label else 0 for t in y_true])\n","    y_pred_bin = np.array([1 if p==positive_label else 0 for p in y_pred])\n","\n","    TP = int(((y_true_bin==1) & (y_pred_bin==1)).sum())\n","    TN = int(((y_true_bin==0) & (y_pred_bin==0)).sum())\n","    FP = int(((y_true_bin==0) & (y_pred_bin==1)).sum())\n","    FN = int(((y_true_bin==1) & (y_pred_bin==0)).sum())\n","    cm = np.array([[TN, FP],[FN, TP]])  # [[TN,FP],[FN,TP]]\n","    return cm, TP, TN, FP, FN\n","\n","def accuracy_score(y_true, y_pred):\n","    return float((np.array(y_true) == np.array(y_pred)).mean())\n","\n","def f1_score_binary(y_true, y_pred, positive_label):\n","    cm, TP, TN, FP, FN = confusion_matrix_binary(y_true, y_pred, positive_label)\n","    precision = TP / (TP + FP + 1e-12)\n","    recall    = TP / (TP + FN + 1e-12)\n","    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n","    return float(f1), float(precision), float(recall)\n","\n","def pretty_print_metrics(title, acc, f1, precision, recall, cm, labels=(\"normal\",\"damaged\")):\n","    print(\"\\n\" + \"=\"*60)\n","    print(title)\n","    print(\"=\"*60)\n","    print(f\"Accuracy : {acc:.4f}\")\n","    print(f\"F1 (pos=damaged) : {f1:.4f}   (Precision {precision:.4f}, Recall {recall:.4f})\")\n","    print(\"\\nConfusion Matrix (rows=true, cols=pred)\")\n","    print(f\"          pred {labels[0]:>8}   pred {labels[1]:>8}\")\n","    print(f\"true {labels[0]:>8}   {cm[0,0]:8d}      {cm[0,1]:8d}\")\n","    print(f\"true {labels[1]:>8}   {cm[1,0]:8d}      {cm[1,1]:8d}\")\n","\n","# =========================\n","# 7-A. YOLO 분류모델 test 예측 + 지표\n","# =========================\n","# YOLO predict는 폴더를 넣으면 각 이미지별 top1 예측을 줍니다.\n","# test 폴더 내부의 모든 이미지를 모아서 예측\n","test_img_paths = list((CLS_DATASET/\"test\").rglob(\"*\"))\n","test_img_paths = [p for p in test_img_paths if p.suffix.lower() in IMG_EXTS]\n","\n","def get_true_label_from_path(p: Path):\n","    # .../test/normal/xxx.jpg 또는 .../test/damaged/xxx.jpg\n","    parts = p.parts\n","    # 마지막에서 두번째가 class 폴더라고 가정\n","    return parts[-2]\n","\n","# YOLO best weights 로딩\n","yolo_cls_best = YOLO(str(yolo_best))\n","\n","y_true_yolo = []\n","y_pred_yolo = []\n","\n","# 배치 예측(속도 위해)\n","yolo_pred = yolo_cls_best.predict([str(p) for p in test_img_paths], verbose=False)\n","for p, r in zip(test_img_paths, yolo_pred):\n","    true = get_true_label_from_path(p)\n","    # r.probs.top1 == 클래스 index\n","    top1 = int(r.probs.top1)\n","    pred_name = r.names[top1]\n","    y_true_yolo.append(true)\n","    y_pred_yolo.append(pred_name)\n","\n","acc_yolo = accuracy_score(y_true_yolo, y_pred_yolo)\n","f1_yolo, prec_yolo, rec_yolo = f1_score_binary(y_true_yolo, y_pred_yolo, positive_label=\"damaged\")\n","cm_yolo, TP, TN, FP, FN = confusion_matrix_binary(y_true_yolo, y_pred_yolo, positive_label=\"damaged\")\n","\n","pretty_print_metrics(\"[YOLO-CLS] Test metrics\", acc_yolo, f1_yolo, prec_yolo, rec_yolo, cm_yolo, labels=(\"normal\",\"damaged\"))\n","\n","# =========================\n","# 7-B. ResNet test 예측 + 지표\n","# =========================\n","# best 모델 로딩\n","resnet.load_state_dict(torch.load(best_path, map_location=DEVICE))\n","resnet.eval()\n","\n","# ImageFolder(test)는 샘플 순서와 (이미지경로,라벨) 매핑을 제공\n","test_samples = test_ds.samples  # [(path, class_idx), ...]\n","\n","y_true_res = []\n","y_pred_res = []\n","\n","with torch.no_grad():\n","    for i in range(0, len(test_ds), RESNET_BATCH):\n","        batch = [test_ds[j] for j in range(i, min(i+RESNET_BATCH, len(test_ds)))]\n","        xs = torch.stack([b[0] for b in batch]).to(DEVICE)\n","        ys = torch.tensor([b[1] for b in batch]).to(DEVICE)\n","        logits = resnet(xs)\n","        preds = logits.argmax(1)\n","\n","        y_true_res += [idx_to_class[int(t)] for t in ys.cpu().numpy().tolist()]\n","        y_pred_res += [idx_to_class[int(p)] for p in preds.cpu().numpy().tolist()]\n","\n","acc_res = accuracy_score(y_true_res, y_pred_res)\n","f1_res, prec_res, rec_res = f1_score_binary(y_true_res, y_pred_res, positive_label=\"damaged\")\n","cm_res, TP, TN, FP, FN = confusion_matrix_binary(y_true_res, y_pred_res, positive_label=\"damaged\")\n","\n","pretty_print_metrics(\"[ResNet18] Test metrics\", acc_res, f1_res, prec_res, rec_res, cm_res, labels=(\"normal\",\"damaged\"))\n","\n","# =========================\n","# 8. TN / FN 샘플 시각화 (각 최대 5개)\n","# - positive = damaged, negative = normal 가정\n","# - TN: true normal & pred normal (정상 맞춘 예)\n","# - FN: true damaged & pred normal (파손을 놓친 예)\n","# =========================\n","def (test_paths, y_true, y_pred, sample_type, max_n=5, pos=\"damaged\", neg=\"normal\"):\n","    picked = []\n","    for p, t, pr in zip(test_paths, y_true, y_pred):\n","        if sample_type == \"TN\":\n","            if (t==neg) and (pr==neg):\n","                picked.append((p, t, pr))\n","        elif sample_type == \"FN\":\n","            if (t==pos) and (pr==neg):\n","                picked.append((p, t, pr))\n","        if len(picked) >= max_n:\n","            break\n","    return picked\n","\n","def show_samples(samples, title):\n","    if len(samples) == 0:\n","        print(f\"\\n[{title}] 샘플이 없습니다.\")\n","        return\n","    n = len(samples)\n","    plt.figure(figsize=(4*n, 4))\n","    for i, (p, t, pr) in enumerate(samples, 1):\n","        img = Image.open(p).convert(\"RGB\")\n","        plt.subplot(1, n, i)\n","        plt.imshow(img)\n","        plt.axis(\"off\")\n","        plt.title(f\"{p.name}\\ntrue={t}, pred={pr}\")\n","    plt.suptitle(title)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# YOLO용 TN/FN\n","tn_yolo = pick_samples_by_type(test_img_paths, y_true_yolo, y_pred_yolo, \"TN\", max_n=5)\n","fn_yolo = pick_samples_by_type(test_img_paths, y_true_yolo, y_pred_yolo, \"FN\", max_n=5)\n","\n","show_samples(tn_yolo, \"[YOLO-CLS] TN samples (max 5)\")\n","show_samples(fn_yolo, \"[YOLO-CLS] FN samples (max 5)\")\n","\n","# ResNet용 TN/FN\n","# ResNet은 test_ds.samples의 path 순서와 y_true_res/y_pred_res가 일치\n","test_paths_res = [Path(p) for p, _ in test_samples]\n","tn_res = pick_samples_by_type(test_paths_res, y_true_res, y_pred_res, \"TN\", max_n=5)\n","fn_res = pick_samples_by_type(test_paths_res, y_true_res, y_pred_res, \"FN\", max_n=5)\n","\n","show_samples(tn_res, \"[ResNet18] TN samples (max 5)\")\n","show_samples(fn_res, \"[ResNet18] FN samples (max 5)\")\n","\n","# =========================\n","# 9. 참고: 자주 조정하는 포인트\n","# =========================\n","# (1) 필터가 너무 엄격해서 데이터가 적으면:\n","#     - MIN_CAR_RATIO = 0.4 또는 0.3 으로 낮추기\n","#     - VEHICLE_NAMES에 'person' 같은 걸 넣지 말고 차량 클래스만 유지\n","#\n","# (2) YOLO 탐지 성능이 부족하면:\n","#     - YOLO_DET_WEIGHTS를 yolov8s.pt / yolov8m.pt로 변경(정확도↑, 속도↓)\n","#\n","# (3) 분류 성능이 부족하면:\n","#     - YOLO_CLS_WEIGHTS를 yolov8s-cls.pt로 변경\n","#     - YOLO_EPOCHS/RESNET_EPOCHS 증가\n","#     - 이미지 증강(RandomRotation, ColorJitter 등) 추가\n","#\n","# (4) TN은 '틀린 이미지'가 아니라 '정상 맞춘 이미지'입니다.\n","#     사용자가 원하신 \"두 유형\"을 그대로 보여드리기 위해 TN/FN을 시각화했습니다.\n","#     만약 \"틀린 것만\" 보려면 FP/FN을 시각화하도록 바꾸면 됩니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fesgP0TVJOII"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNBVnpS9hfL1eMVLFqf9R6F"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}